{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devendra-D-Umbrajkar/NLP-Assignments/blob/main/Assignment_1_for_DS_207_(Intro_to_NLP)_Word_Representation_and_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHRmq-UanuC"
      },
      "source": [
        "# Assignment 1: Word Representation and Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you will explore the different types of text representations, similarity functions and tackle the problem of multi-class text classification. Please make a copy of this notebook (locally or on Colab). Ensure you adhere to the guidelines and submission instructions (mentioned below) for attempting and submitting the assignment.\n",
        "\n",
        "One of the tasks in this assignment is compute intensive, and is better performed on an accelerator device (GPU, etc.). Unless you have one locally, prefer using a GPU instance on Colab for execution."
      ],
      "metadata": {
        "id": "9CWjuzEsLkdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guidelines for Attempting the Assignment\n"
      ],
      "metadata": {
        "id": "g_EzAmepKF4B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaXuoUpf2qgU"
      },
      "source": [
        "1. Please read the function docs and comments carefully. They provide specific instructions and examples for implementing each function. Follow these instructions precisely - neither oversimplify nor overcomplicate your implementations. Deviating from the provided implementation guidelines may result in lost marks.\n",
        "\n",
        "2. Write your logic in the cells which have the comment `# ADD YOUR CODE HERE`, between the `# BEGIN CODE` and `# END CODE` comments. These cells are also demarcated by the special start (`# ==== BEGIN EVALUATION PORTION`) and end (`# ==== END EVALUATION PORTION`) comments.\n",
        "\n",
        "3. Do **NOT** remove any of these comments from the designated cells, otherwise your assignment may not be evaluated correctly.\n",
        "\n",
        "4. All imports that should be necessary are already provided as part of the notebook. Should you require additional imports, add them in the cells to be graded, but outside the `# BEGIN CODE` and `# END CODE` block. For example, if you need to import a package called `mypackage`, add it as follows in a graded cell:\n",
        "\n",
        "``` python\n",
        "## ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "import mypackage # <===\n",
        "\n",
        "def function_to_be_implemented(*args, **kwargs):\n",
        "\n",
        "    ...\n",
        "\n",
        "    # BEGIN CODE: block.segment\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    # END CODE\n",
        "\n",
        "    ...\n",
        "\n",
        "## ==== END EVALUATION PORTION\n",
        "\n",
        "```\n",
        "5. Do not modify anything in the cells which start with `# Please do not change anything in the following cell`.\n",
        "\n",
        "6. Ensure you only add code in designated areas, otherwise you assignment will not be evaluated. If you encounter any errors in the supporting cells during execution, contact the respective TAs.\n",
        "\n",
        "7. Ensure that the total runtime of the assignment is less than 20 minutes. Exceeding this time may result in deductions of marks.\n",
        "\n",
        "8. **Important**: Use of AI-assistive technologies such as ChatGPT or GitHub CoPilot is not permitted for this assignment. Ensure that all attempts are solely your own. Not following this rule can incur heavy penalty, including getting NO GRADE for this assignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission Instructions\n",
        "\n",
        "1. When you have completely attempted the assignment, **export the current notebook as a `.py` file**, with the following name: `SAPName_SRNo_assignment1.py`, where `SAPName` would be your name as per SAP record, and `SRNo` will be the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use `Twyla_Linda_15329_assignment1.py`.\n",
        "\n",
        "2. You should put your assignment file `SAPName_SRNo_assignment1.py` inside a folder `SAPName_SRNo`. The folder structure looks as follows:\n",
        "\n",
        "``` python\n",
        "‚îî‚îÄ‚îÄ‚îÄ SAPName_SRNo\n",
        "     ‚îú‚îÄ‚îÄ‚îÄ SAPName_SRNo_assignment1.py\n",
        "```\n",
        "\n",
        "3. Once you have validated the folder structure as above, zip the folder and name it as `submission.zip` and submit this ZIP archive.\n",
        "\n",
        "4. When you run the assignment code, it may download certain datasets and other artifacts. These should **NOT** be part of the above folder.\n",
        "\n",
        "\n",
        "**If you have any confusion regarding submission instructions, please ask the respective TAs.**"
      ],
      "metadata": {
        "id": "kmbvifRqKfJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Marks Distribution\n",
        "\n",
        "- Generative Classification: 30 marks\n",
        "- SkipGram Training and Word2Vec: 40 marks\n",
        "- Discriminative Classification: 30 marks"
      ],
      "metadata": {
        "id": "RHnyldiEK0Zo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxlIbIcx2ucZ"
      },
      "source": [
        "In the cell below, replace `SAP_Name` with your name as per SAP record, and `SRNo` with the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use:\n",
        "\n",
        "```python\n",
        "STUDENT_SAP_NAME  = \"Twyla_Linda\"\n",
        "STUDENT_SR_NUMBER = \"15329\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17rh5p0U2twH"
      },
      "outputs": [],
      "source": [
        "STUDENT_SAP_NAME  = \"SAP_Name\"\n",
        "STUDENT_SR_NUMBER = \"SRNo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCHG0bmuhUuq"
      },
      "source": [
        "# üóûÔ∏è Timothy and the Impossible Inbox\n",
        "\n",
        "Every morning at The Daily Byte, one person opens an inbox that should not exist. That person is Timothy\n",
        "\n",
        "Timothy works at one of the biggest news groups in the world. While the newsroom has 5 departments, hundreds of sub-departments, thousands of journalists, and countless dashboards, the very first stop for every article is Timothy's screen. Politics. Sports. Technology. Business. Entertainment. Thousands of articles arrive each day that need to be redirected to the right department.\n",
        "\n",
        "Timothy reads headlines at the speed of light, skims paragraphs faster than most people blink, and still falls behind before lunch. Manual redirection is not just inefficient, it is impossible. The inbox grows. Deadlines glare. Coffee cools untouched.\n",
        "\n",
        "Timothy is curious, stubborn, and slightly allergic to doing the same task twice. Faced with an inbox that grows faster than human attention, Timothy decides to automate the problem. Late nights find Timothy reading articles on Natural Language Processing. Some papers talk about simple probabilistic models. Others describe dense word representations and neural networks. The ideas are promising, but the implementations are unfinished, messy, or missing entirely.\n",
        "\n",
        "Timothy sketches pipelines on sticky notes:\n",
        "\n",
        "- ‚ÄúCan machines decide where an article belongs?‚Äù\n",
        "\n",
        "The theory is clear. The execution is not. That is when Timothy turns to you.\n",
        "\n",
        "**You are part of Timothy's newly formed NLP task force.**\n",
        "\n",
        "Timothy understands the ideas, but needs help turning theory into working systems. Your job is to implement, train, test, and evaluate the models Timothy has been reading about. Each section of this assignment corresponds to a method Timothy discovered in the literature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7V2xJdCanuc"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "ZpPwStZrPIa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NIu94tsWanud"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import argparse\n",
        "import sys\n",
        "import joblib\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s71PAyk-dRp9"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Timothy has collected a dataset of ~1800 news articles from The Daily Byte and has shared it with the team. The dataset consists of news articles categorized into five mutually exclusive classes: Politics, Entertainment, Sports, Business, and Science/Technology. Each data instance contains a short news article in the field \"text\", and labels are encoded as integers from 0 to 4 corresponding to the five news categories, in the field \"label\".\n",
        "\n",
        "Timothy has split the dataset into a labeled training set (train.csv) and a validation set (val.csv) with balanced classes. You will use the training set for training your classifiers and testing set to test their performance.\n",
        "\n",
        "You can run the below code snippets to download and load the training and testing sets as Pandas Dataframes. If you are new to Pandas, you can read more about it [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iltgG_HGKBZE"
      },
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U85z2ycf8m6j"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "file_ids = {\n",
        "    \"train\": \"1SPmPJ4OQdurBP-uLUwMoynKa7ZhH1iDk\",\n",
        "    \"test\": \"1imCHe_htxStA3JLXYM_mSn3-HnLO96zQ\",\n",
        "}\n",
        "\n",
        "url_template = \"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Training and Testing Data with News articles\n",
        "train_output = \"train.csv\"\n",
        "test_output = \"test.csv\"\n",
        "\n",
        "gdown.download(url_template.format(file_id=file_ids['train']), train_output, quiet=False)\n",
        "gdown.download(url_template.format(file_id=file_ids['test']), test_output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRX6Ucs3VAF"
      },
      "source": [
        "In case you see an error like -\n",
        "```\n",
        "package not found gdown\n",
        "```\n",
        "Restart the session and you should be fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syDYBbIC8Crq"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_output)\n",
        "test_df = pd.read_csv(test_output)\n",
        "\n",
        "print(\"Number of training samples - \", len(train_df))\n",
        "print(\"Number of testing samples - \", len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfWPim4o1JxT"
      },
      "source": [
        "# 1. Generative Modeling with Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAR6vMplanuk"
      },
      "source": [
        "\n",
        "Timothy finds a book that describes text classification as a **generation problem**. Given a document and a set of possible classes it can belong to, they ask\n",
        "\n",
        "> *Which class was most likely to generate/have this document?*\n",
        "\n",
        "This idea leads Timothy to **Naive Bayes**.\n",
        "\n",
        "Let:\n",
        "\n",
        "- Let $X = (w_1, w_2, \\dots, w_n)$ be a document or a piece of text with multiple words\n",
        "- Let $y$ be a class (Politics, Sports, Technology, etc.)\n",
        "\n",
        "\n",
        "Using Bayes' Theorem:\n",
        "\n",
        "$$\n",
        "P(y \\mid X) = \\frac{P(X \\mid y) \\space P(y)}{P(X)}\n",
        "$$\n",
        "\n",
        "Since $ P(d) $ is constant across classes, Timothy only needs to compute:\n",
        "\n",
        "$$\n",
        "P(y \\mid X) \\propto P(y) \\space P(X \\mid y)\n",
        "$$\n",
        "\n",
        "<!-- where\n",
        "- $P(c \\mid d)$ is the posterior probability\n",
        "- $P(c)$ is the prior probability of the class\n",
        "- $P(d \\mid c)$ is the likelihood of the document given the class -->\n",
        "\n",
        "### The ‚ÄúNaive‚Äù Assumption\n",
        "\n",
        "Naive Bayes assumes **conditional independence of words** (This assumption is clearly unrealistic. Words influence each other. Hence the name \"naive\"):\n",
        "\n",
        "$$\n",
        "P(X \\mid y) = \\prod_{i=1}^{n} P(w_i \\mid y)\n",
        "$$\n",
        "\n",
        "Hence using both the equations\n",
        "\n",
        "$$\n",
        "P(X \\mid y) \\propto P(y) \\space \\prod_{i=1}^{n} P(w_i \\mid y)\n",
        "$$\n",
        "\n",
        "In a computer system, if the probabilities are too low and/or the document has a large number of words, it can lead to numerical underflow (think about why). **Log space** is used to prevent numerical underflow:\n",
        "\n",
        "$$\n",
        "log(P(X \\mid y)) \\propto log(P(y)) + \\sum_{i=1}^{n} log(P(w_i \\mid y))\n",
        "$$\n",
        "\n",
        "Timothy further reads that a special token called **UNK token** is used for words that are not found in the vocabulary but are encountered during prediction!\n",
        "\n",
        "Timothy shares the book with you. You find that some parts of the code in the book is missing. Complete the below code to implement **Generative Classification using Naive Bayes**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Create a Vocabulary"
      ],
      "metadata": {
        "id": "6MXRYBT_WaZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Before working with any text data, we need to clean the data and remove all noisy elements.\n",
        "    In this function, we preprocess the input text by converting everything to lowercase,\n",
        "    removing non-word characters and filtering out stop words.\n",
        "\n",
        "    The preprocessing pipeline performs the following steps:\n",
        "    1. Converts the text to lowercase.\n",
        "    2. Removes punctuation and non-word characters.\n",
        "    3. Splits the text into individual words.\n",
        "    4. Filters out common English stop words (e.g., 'a', 'an', 'the', 'is', 'are', 'how')\n",
        "        using NLTK's stop word list.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw input text\n",
        "            Example: \"Hello, World! How are you doing today?\"\n",
        "\n",
        "    Returns:\n",
        "        list: List of cleaned, tokenized, and filtered words with stop words removed\n",
        "            Example: ['hello', 'world', 'doing', 'today'\n",
        "    \"\"\"\n",
        "    # Import stop words from NLTK\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Convert the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Extract words and split into tokens\n",
        "    tokens = re.findall(r'\\w+', text)\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def create_vocabulary(texts, min_freq = 1):\n",
        "    \"\"\"\n",
        "    As discussed in class, we create vocabulary from training texts by mapping unique words to indices.\n",
        "    And to reduce noise, we only include words in the vocabulary that have been encountered at least\n",
        "    min_freq number of times in the text.\n",
        "\n",
        "    Complete this method to create a vocabulary from a given list of texts. The vocabulary construction\n",
        "    process must include:\n",
        "      1. Preprocessing each document using `preprocess_text`.\n",
        "      2. Counting word frequencies across the entire corpus and only including words that appear >= min_freq times\n",
        "      5. `<UNK>` token must be present at index 0. This will be used to map all OOV (Out of Vocabulary) words\n",
        "          during evaluation.\n",
        "\n",
        "    Args:\n",
        "        texts (list): List of text documents\n",
        "            Example: [\n",
        "                    \"Hello world hello\",\n",
        "                    \"Hello there\",\n",
        "                    \"World is beautiful\"\n",
        "            ]\n",
        "    Returns:\n",
        "        dict: Mapping of words to unique indices, including UNK token\n",
        "            Example (with min_freq=2): {\n",
        "                '<UNK>': 0,    # Special token for rare/unseen words\n",
        "                'hello': 1,    # Frequency=3, included in vocab\n",
        "                'world': 2,    # Frequency=2, included in vocab\n",
        "                # 'there' and 'beautiful' not included (frequency=1 < min_freq=2)\n",
        "            }\n",
        "    \"\"\"\n",
        "\n",
        "    # BEGIN CODE : preprocess_and_create_vocab\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END CODE\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ],
      "metadata": {
        "id": "lC2zK6RLL9Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Train a Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "QnsaKaNlXWnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVaWWK5Tanul"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Here we initialize the Naive Bayes classifier.\n",
        "\n",
        "        Args:\n",
        "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
        "                           Words appearing less than min_freq times will be treated as UNK token.\n",
        "                           Default: 1 (include all words)\n",
        "\n",
        "        Attributes:\n",
        "            class_probs (dict): P(y) for each class\n",
        "                Example: {0: 0.5, 1: 0.5}\n",
        "\n",
        "            word_probs (dict): P(X|y) for each word and class\n",
        "                Example: {\n",
        "                    1: {'cricket': 0.2, 'computer': 0.3, '<UNK>': 0.1},\n",
        "                    2: {'cricket': 0.9, 'computer': 0.1, '<UNK>': 0.1},\n",
        "                    3: {'cricket': 0.3, 'computer': 0.6, '<UNK>': 0.1},\n",
        "                    4: {'cricket': 0.1, 'computer': 0.9, '<UNK>': 0.1}\n",
        "                }\n",
        "\n",
        "            vocabulary (dict): Word to index mapping, including special UNK token\n",
        "                Example: {'<UNK>': 0, 'cricket': 1, 'computer': 2}\n",
        "\n",
        "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
        "        \"\"\"\n",
        "        self.class_probs = None\n",
        "        self.word_probs = None\n",
        "        self.vocabulary = None\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def calculate_class_probabilities(self, labels):\n",
        "        \"\"\"\n",
        "        Complete this function to estimate probability P(y) for each class from training labels.\n",
        "        Recall that for Naive Bayes event A, P(A) = Number of occurrences of A / Total Samples\n",
        "        This code should handle ANY number of unique classes.\n",
        "\n",
        "        Sanity Check:\n",
        "            - Probabilities must sum to 1 across all classes\n",
        "\n",
        "        Args:\n",
        "            labels (list): List of class labels\n",
        "                Example: [3, 0, 1, 2, 0, 1]\n",
        "\n",
        "        Returns:\n",
        "            dict: Estimated probability for each class\n",
        "                Example: {\n",
        "                    0: 0.33,     # 2 out of 6 samples are class 0\n",
        "                    1: 0.33,     # 2 out of 6 samples are class 1\n",
        "                    2: 0.17,     # 1 out of 6 samples are class 2\n",
        "                    3: 0.17,     # 1 out of 6 samples are class 3\n",
        "                    4: 0.00      # 0 out of 6 samples are class 4\n",
        "                }\n",
        "\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : naive_bayes.calculate_class_probabilities\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def calculate_word_probabilities(self, texts, labels, vocabulary, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Complete this function to calculate conditional probability P(X|y) for each word X and class y,\n",
        "        including probability for <UNK> token.\n",
        "\n",
        "        The vocabulary construction process must include:\n",
        "          1. Preprocess each document using `preprocess_text`.\n",
        "          2. Replace OOV words (words not in vocabulary) with UNK token.\n",
        "          3. Use Laplace smoothing to handle unseen words\n",
        "            P(X|y) = (count(X,y) + Œ±) / (total_words_in_class + Œ±|V|)\n",
        "            where |V| is vocabulary size (including <UNK> token)\n",
        "\n",
        "\n",
        "        Args:\n",
        "            texts (np.array): Document-term matrix (with <UNK> counts in first column)\n",
        "                Example: array([\n",
        "                    [0, 2, 1],  # Document 1: 0 <UNK>s, 2 of word 1, 1 of word 2\n",
        "                    [1, 0, 1],  # Document 2: 1 <UNK>, 0 of word 1, 1 of word 2\n",
        "                ])\n",
        "            labels (list): Class labels\n",
        "                Example: [0, 1]\n",
        "            vocabulary (dict): Word to index mapping with <UNK> token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "            alpha (float): Laplace smoothing parameter, default=1.0\n",
        "\n",
        "        Returns:\n",
        "            dict: Nested dict with P(X|y) for each word X and class y\n",
        "                Example: {\n",
        "                    0: {\n",
        "                        '<UNK>': 0.167,    # P(X=<UNK>|y=0)\n",
        "                        'hello': 0.5,      # P(X='hello'|y=0)\n",
        "                        'world': 0.333     # P(X='world'|y=0)\n",
        "                    },\n",
        "                    1: {\n",
        "                        '<UNK>': 0.4,     # P(X=<UNK>|y=1)\n",
        "                        'hello': 0.2,     # P(X='hello'|y=1)\n",
        "                        'world': 0.4      # P(X='world'|y=1)\n",
        "                    }\n",
        "                }\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : naive_bayes.calculate_word_probabilities\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def fit(self, texts, labels):\n",
        "        \"\"\"\n",
        "        In the function we train the Naive Bayes classifier on the provided text documents.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\n",
        "                    \"Kohli played well\",\n",
        "                    \"Stock markets crashed last sunday\",\n",
        "                    \"Harry Styles's new song is out\"\n",
        "                ]\n",
        "            labels (list): Class labels\n",
        "                Example: [2, 1, 3]\n",
        "        \"\"\"\n",
        "        # Create vocabulary from training texts\n",
        "        self.vocabulary = create_vocabulary(texts, self.min_freq)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        self.class_probs = self.calculate_class_probabilities(labels)\n",
        "        self.word_probs = self.calculate_word_probabilities(texts, labels, self.vocabulary)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        \"\"\"\n",
        "        Complete this function to predict classes for new documents using Naive Bayes algorithm,\n",
        "        handling unknown words using <UNK> token.\n",
        "\n",
        "        For each document:\n",
        "            1. Preprocess the text using ``preprocess_text``.\n",
        "            2. Replace OOV words (out-of-vocabulary or words not in vocabulary) with UNK token.\n",
        "            3. Calculate log probabilities using appropriate word or UNK probabilities.\n",
        "               UNK probability is used for OOV (out-of-vocabulary) words.\n",
        "            4. Return class with highest log probability score.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\n",
        "                    \"Kohli played well\",\n",
        "                    \"Stock markets crashed last sunday\",\n",
        "                    \"Harry Styles's new song is out\"\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [2, 1, 3]\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : naive_bayes.predict\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def get_important_words(self, n=5, use_ratio=True):\n",
        "        \"\"\"\n",
        "        In the function we get the most important words for each class based either on their raw conditional\n",
        "        probabilities or their probability ratios between classes.\n",
        "\n",
        "        Args:\n",
        "            n (int): Number of top words to return for each class, default=5\n",
        "            use_ratio (bool): If True, ranks words by probability ratio between classes\n",
        "                            If False, ranks words by raw conditional probability\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping class labels to lists of (word, score) tuples,\n",
        "                where score is either probability or probability ratio\n",
        "\n",
        "                Example with use_ratio=False:\n",
        "                {\n",
        "                    0: [('excellent', 0.014), ('great', 0.012), ('amazing', 0.011)],\n",
        "                    1: [('terrible', 0.015), ('bad', 0.012), ('boring', 0.008)],\n",
        "                    2: [('politics', 0.018), ('election', 0.014), ('policy', 0.010)]\n",
        "                }\n",
        "\n",
        "                Example with use_ratio=True:\n",
        "                {\n",
        "                    0: [('excellent', 6.8), ('amazing', 5.9), ('great', 5.2)],\n",
        "                    1: [('terrible', 7.4), ('awful', 6.3), ('bad', 5.7)],\n",
        "                    2: [('election', 8.1), ('policy', 6.9), ('vote', 6.2)]\n",
        "                }\n",
        "\n",
        "    Notes:\n",
        "        - When use_ratio=True:\n",
        "            - For each class c, words are ranked by:\n",
        "                P(word | c) / average P(word | other classes)\n",
        "            - This highlights words that are distinctive for a class relative to all other classes.\n",
        "            - This helps reduce overlap between top words across classes.\n",
        "        - When use_ratio=False:\n",
        "            - Words are ranked by raw P(word | class).\n",
        "            - Common words may appear in multiple classes.\n",
        "        - Includes the UNK token only if it ranks among the top n words.\n",
        "        - Small probabilities are handled safely to avoid division by zero.\n",
        "        \"\"\"\n",
        "        if not self.word_probs:\n",
        "            raise ValueError(\"Classifier must be trained before getting important words\")\n",
        "\n",
        "        important_words = {}\n",
        "        classes = sorted(self.word_probs.keys())\n",
        "\n",
        "        for cls in classes:\n",
        "            # Get next class\n",
        "            other_classes = [c for c in classes if c != cls]\n",
        "\n",
        "            if use_ratio:\n",
        "                # Probability ratios for all words\n",
        "                word_scores = []\n",
        "                for word in self.vocabulary:\n",
        "                    numerator = self.word_probs[cls][word]\n",
        "                    denominator = sum(self.word_probs[c][word] for c in other_classes) / len(other_classes)\n",
        "                    ratio = numerator / (denominator + 1e-4)\n",
        "                    word_scores.append((word, ratio))\n",
        "            else:\n",
        "                # Use raw probabilities\n",
        "                word_scores = list(self.word_probs[cls].items())\n",
        "\n",
        "            # Sort by score (either ratio or probability) and take top n\n",
        "            sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "            important_words[cls] = sorted_words[:n]\n",
        "\n",
        "        return important_words\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNBmM1FXgbuv"
      },
      "source": [
        "### 1.3 Evaluate the Naive Bayes Model\n",
        "\n",
        "Before sharing the code with Timothy, you want to verify if it runs. You note down a few sentences related to each of the 5 departments and check if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTIct4vX4adn"
      },
      "outputs": [],
      "source": [
        "# Minimum frequency of words to be present in the vocabulary.\n",
        "# You can play with the value of min_freq and notice how the top words converge for larger values\n",
        "# and <UNK> dominates. This is because we are testing in a tiny universe.\n",
        "MIN_FREQ = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjulkO45anur"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_sample_data(use_ratio=False):\n",
        "\n",
        "    X_train = [\n",
        "      # ---- Tech (0) ----\n",
        "      \"AI neural network improves deep learning accuracy\",\n",
        "      \"Quantum computing processor accelerates machine learning tasks\",\n",
        "      \"Cybersecurity researchers detect ransomware malware attack\",\n",
        "      \"Software engineers release open source encryption library\",\n",
        "      \"Cloud computing platform optimizes data center performance\",\n",
        "\n",
        "      # ---- Business (1) ----\n",
        "      \"Stock market investors react to quarterly earnings report\",\n",
        "      \"Corporate merger increases company valuation and revenue\",\n",
        "      \"Bank reports profit growth amid rising interest rates\",\n",
        "      \"Shareholders approve acquisition during annual meeting\",\n",
        "      \"Financial analysts predict economic slowdown and inflation\",\n",
        "\n",
        "      # ---- Sport (2) ----\n",
        "      \"Football team wins championship after penalty shootout\",\n",
        "      \"Cricket batsman scores century in international match\",\n",
        "      \"Olympic athlete suffers injury before major tournament\",\n",
        "      \"Coach announces squad for upcoming league season\",\n",
        "      \"Referee awards red card during intense derby match\",\n",
        "\n",
        "      # ---- Entertainment (3) ----\n",
        "      \"Hollywood movie premieres at international film festival\",\n",
        "      \"Actor receives award for outstanding performance\",\n",
        "      \"Pop singer releases album topping music charts\",\n",
        "      \"Streaming series gains popularity among young audiences\",\n",
        "      \"Director praises cast during cinema award ceremony\",\n",
        "\n",
        "      # ---- Politics (4) ----\n",
        "      \"Parliament passes bill after heated legislative debate\",\n",
        "      \"Election campaign focuses on healthcare and education policy\",\n",
        "      \"Government announces new foreign policy strategy\",\n",
        "      \"Opposition party criticizes tax reform proposal\",\n",
        "      \"Prime minister addresses nation on climate change\"\n",
        "    ]\n",
        "\n",
        "    y_train = (\n",
        "        [0] * 5 +\n",
        "        [1] * 5 +\n",
        "        [2] * 5 +\n",
        "        [3] * 5 +\n",
        "        [4] * 5\n",
        "    )\n",
        "\n",
        "    X_val = [\n",
        "        \"Breakthrough in artificial intelligence research\",\n",
        "        \"Company shares fall after profit warning\",\n",
        "        \"Final match ends in dramatic penalty shootout\",\n",
        "        \"Actor wins award for leading role\",\n",
        "        \"Government announces new foreign policy strategy\"\n",
        "    ]\n",
        "\n",
        "    y_val = [0, 1, 2, 3, 4]\n",
        "\n",
        "    # Training classifier\n",
        "    nb_classifier = NaiveBayesClassifier(min_freq=MIN_FREQ)\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Prediction\n",
        "    predictions = nb_classifier.predict(X_val)\n",
        "\n",
        "    # Evaluation\n",
        "    accuracy = sum(int(p == y) for p, y in zip(predictions, y_val)) / len(y_val)\n",
        "    print(f\"Validation accuracy on the dummy dataset: {accuracy:.4f}\")\n",
        "\n",
        "    # Top words per class\n",
        "    class_names = {\n",
        "        0: \"Tech\",\n",
        "        1: \"Business\",\n",
        "        2: \"Sport\",\n",
        "        3: \"Entertainment\",\n",
        "        4: \"Politics\"\n",
        "    }\n",
        "\n",
        "    important_words = nb_classifier.get_important_words(n=5, use_ratio=use_ratio)\n",
        "\n",
        "    for class_label, words in important_words.items():\n",
        "        print(f\"\\nTop words for {class_names[class_label]}:\")\n",
        "        for word, prob in words:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "# Don't worry about the accuracy here.\n",
        "train_and_evaluate_sample_data(use_ratio=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Does <UNK> dominate the results. Don't worry, this is expected.\n",
        "# Now let's see how the top performing words change when we use the ratio instead of raw\n",
        "# probabilities to compare.\n",
        "train_and_evaluate_sample_data(use_ratio=True)"
      ],
      "metadata": {
        "id": "fcLvc59SacxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like you are ready to test your model on the Timothy's dataset!"
      ],
      "metadata": {
        "id": "IrKMIdk_byXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the following evaluation on a much larger universe,\n",
        "# you can use a larger value of min_freq.\n",
        "# Notice how min_freq affects the accuracy and top words.\n",
        "MIN_FREQ = False"
      ],
      "metadata": {
        "id": "LSDNx8TFtCxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98iQZI72Nw4n"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_test_data(train_df, test_df, use_ratio=False):\n",
        "    \"\"\"\n",
        "    Train and evaluate NaiveBayesClassifier on the AG News dataset.\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to train.csv\n",
        "        val_path (str): Path to val.csv\n",
        "    \"\"\"\n",
        "\n",
        "    X_train = train_df['text'].tolist()\n",
        "    y_train = train_df['label'].tolist()\n",
        "    X_test = test_df['text'].tolist()\n",
        "    y_test = test_df['label'].tolist()\n",
        "\n",
        "    # Initialize and train classifier\n",
        "    nb_classifier = NaiveBayesClassifier(min_freq=MIN_FREQ)\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on validation set\n",
        "    predictions = nb_classifier.predict(X_test)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = sum(int(p == y) for p, y in zip(predictions, y_test)) / len(y_test)\n",
        "    print(f\"Validation Accuracy on AG News: {accuracy:.4f}\")\n",
        "\n",
        "    # Display top words for each class\n",
        "    class_names = {\n",
        "        0: \"Tech\",\n",
        "        1: \"Business\",\n",
        "        2: \"Sport\",\n",
        "        3: \"Entertainment\",\n",
        "        4: \"Politics\"\n",
        "    }\n",
        "    important_words = nb_classifier.get_important_words(n=5, use_ratio=use_ratio)\n",
        "\n",
        "    for class_label, words in important_words.items():\n",
        "        print(f\"\\nTop words for {class_names.get(class_label, class_label)}:\")\n",
        "        for word, prob in words:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "# Accuracy above 80% is good.\n",
        "train_and_evaluate_test_data(train_df, test_df, use_ratio=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Again, let's see how the top performing words change when we use the ratio instead of raw\n",
        "# probabilities to compare.\n",
        "# Accuracy above 80% is good.\n",
        "train_and_evaluate_test_data(train_df, test_df, use_ratio=True)"
      ],
      "metadata": {
        "id": "C_wCLXEJb-Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timothy is happy with your results. But he is not convinced that this is the best way. So he continues digging."
      ],
      "metadata": {
        "id": "floLqsjucRmF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jeKEN6I1M_h"
      },
      "source": [
        "# 2. Training your own Word2Vec Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXX1CUYaanuv"
      },
      "source": [
        "\n",
        "Timothy found that some papers argue that **meaning is not counted, but inferred**. Words mean what they do because of the words they appear near. This leads Timothy to **Word2Vec**.\n",
        "\n",
        "### Skip-Gram Training\n",
        "\n",
        "For target word $w_t$ we predict its surrounding context words $w_c$. Given a center word $w_t$ and a context window $n$,the objective is to maximise the likelihood of observing surrounding words in the range $w_{t-j}$ to $w_{t+j}$\n",
        "$$\n",
        "maximise \\sum_{t=1}^{T} \\sum_{w_c \\in \\text{Context}(t)} \\log P(w_c \\mid w_t ; \\theta) \\\\\n",
        "= maximise \\sum_{t=1}^{T} \\sum_{-n \\le j \\le n \\\\ j\\ne 0} \\log P(w_{t+j} \\mid w_t ; \\theta)\n",
        "$$\n",
        "\n",
        "Word2Vec represents each word as a vector.\n",
        "\n",
        "Each word $w$ in the vocabulary $V$ is associated with two vectors - the center embedding ($u_w$) and the context/outer embedding ($v_w$). These are stored as two embedding matrices in code. The probability of a context word given a target word is modeled using softmax.\n",
        "\n",
        "$$\n",
        "P(w_c \\mid w_t) = P(o \\mid c) =\n",
        "\\frac{\\exp(u_o^T \\space v_c)}\n",
        "{\\sum_{w \\in V} \\exp(u_w^T \\space v_c)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $w_t$ = c: center word\n",
        "* $w_c$ = o: context/outer words\n",
        "* $v_c$: embedding of the center word\n",
        "* $u_o$: embedding of a candidate context word\n",
        "* $u_o^\\top v_c$: similarity score (dot product)\n",
        "* Denominator: normalisation over **all words in the vocabulary**\n",
        "\n",
        "### The Computational Bottleneck\n",
        "\n",
        "Timothy quickly notices the trapdoor hidden in the denominator.\n",
        "\n",
        "The softmax requires a summation over the entire vocabulary for every training pair. For vocabularies with tens or hundreds of thousands of words, this becomes computationally impractical. The model knows which word should be close, but is forced to check **every word it should not be close to**.\n",
        "\n",
        "## Negative Sampling\n",
        "\n",
        "Negative Sampling reframes the problem as a set of **binary classification tasks**:\n",
        "\n",
        "> ‚ÄúDid this word actually appear in the context of the center word, or was it just noise?‚Äù\n",
        "\n",
        "Instead of normalizing over the full vocabulary, the model pulls real context words closer and pushes randomly sampled noise words away\n",
        "\n",
        "Let:\n",
        "\n",
        "* $D$: set of positive pairs (true center-context pairs occurring in text)\n",
        "* $D'$: set of negative pairs (randomly sampled word pairs)\n",
        "\n",
        "For each positive pair $(c, o)$, we sample $k$ negative words $o' \\sim P_n(w)$. The training objective becomes:\n",
        "\n",
        "$$\n",
        "\\max_\\theta\n",
        "\\prod_{(c,o)\\in D} \\sigma(u_o^\\top v_c)\n",
        "\\prod_{(c,o')\\in D'} \\sigma(-u_{o'}^\\top v_c)\n",
        "$$\n",
        "\n",
        "Or equivalently, in log space:\n",
        "\n",
        "$$\n",
        "\\max_\\theta\n",
        "\\sum_{(c,o)\\in D} \\log \\sigma(u_o^\\top v_c)\n",
        "\\space +\n",
        "\\sum_{(c,o')\\in D'} \\log \\sigma(-u_{o'}^\\top v_c)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\min_\\theta\n",
        "\\space - \\{\\sum_{(c,o)\\in D} \\log \\sigma(u_o^\\top v_c)\n",
        "\\space +\n",
        "\\sum_{(c,o')\\in D'} \\log \\sigma(-u_{o'}^\\top v_c)\n",
        "\\}\n",
        "$$\n",
        "\n",
        "* $\\sigma(u_o^\\top v_c)$\n",
        "  pushes real context words to have high dot products with the center word\n",
        "\n",
        "* $\\sigma(-u_{o'}^\\top v_c)$\n",
        "  pushes noise words to have low dot products\n",
        "\n",
        "In this exercise, you will code the skipgram model from scratch using the PyTorch library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vTRKLt7tfBX"
      },
      "source": [
        "**NOTE**: Connect to a GPU runtime for this part of the assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Training SkipGram Model with Negative Sampling"
      ],
      "metadata": {
        "id": "8Euvlby40VMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAag2Jg_vQM0"
      },
      "outputs": [],
      "source": [
        "# You can choose and play around with modelling parameters here.\n",
        "# Note that we will be defining our own set of parameters\n",
        "# during evaluation, hence the values used here are not indicative of\n",
        "# the performance of your model.\n",
        "\n",
        "# Number of context words to consider around the center word\n",
        "SKIPGRAM_N_WORDS = False\n",
        "\n",
        "# Only consider words which occur at least MIN_WORD_FREQUENCY times\n",
        "MIN_WORD_FREQUENCY = False\n",
        "\n",
        "# Truncate long sentences to MAX_SEQUENCE_LENGTH\n",
        "MAX_SEQUENCE_LENGTH = False\n",
        "\n",
        "# Embedding dimension\n",
        "EMBED_DIMENSION = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWJ0Tg10vN1C"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenise the text into word in lower case\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r\"\\b\\w+\\b\")\n",
        "    return pattern.findall(text.lower())\n",
        "\n",
        "def load_text(split):\n",
        "    \"\"\"\n",
        "    Load the dataset from HuggingFace.\n",
        "\n",
        "    Args:\n",
        "        split (str): The split to load 'train' or 'validation'\n",
        "\n",
        "    Returns:\n",
        "        list[str]: List of text lines\n",
        "    \"\"\"\n",
        "    if split == \"train\":\n",
        "        dataset1 = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")['train']\n",
        "        dataset2 = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")['test']\n",
        "        return [item['text'] for item in dataset1 if item['text'].strip()] + \\\n",
        "               [item['text'] for item in dataset2 if item['text'].strip()]\n",
        "    elif split == \"validation\":\n",
        "        dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")['validation']\n",
        "        return [item['text'] for item in dataset if item['text'].strip()]\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid split name: {split}\")\n",
        "\n",
        "\n",
        "def collate_skipgram(batch, vocab, max_sequence_length=MAX_SEQUENCE_LENGTH, window=SKIPGRAM_N_WORDS):\n",
        "    \"\"\"\n",
        "    Complete this function to create (center_word, context_word) pairs for Skip-gram training.\n",
        "\n",
        "    For each text in a batch:\n",
        "\n",
        "        - Tokenize the text using ``tokenize``\n",
        "        - Get the IDs of all tokens (words) in the text that are in the vocabulary.\n",
        "           If a word is not in the vocabulary, skip it.\n",
        "        - If the length of the text is > MAX_SEQUENCE_LENGTH, truncate it to MAX_SEQUENCE_LENGTH.\n",
        "        - For center index i and window size j, context indices are [i-j to i-1] and [i+1 to i+j]\n",
        "          inclusive of the ends.\n",
        "        - Create center_words list and context_words list where for every index i, center_words[i]\n",
        "          and context_words[i] form the (center_word, context_word) pair.\n",
        "\n",
        "        - IMPORTANT: Convert center_words and context_words to torch.tensor.\n",
        "          Example - center_words = torch.tensor(center_words, dtype=torch.long)\n",
        "                    context_words = torch.tensor(context_words, dtype=torch.long)\n",
        "        - IMPORTANT: Return (center_words, context_words) as a pair. Changing the return object structure\n",
        "                    may result in loss of marks.\n",
        "\n",
        "    Args:\n",
        "\n",
        "        batch(list): List of text documents.\n",
        "\n",
        "        vocab(dict): A vocabulary object created using ``create_vocabulary``.\n",
        "            It is a dictionary mapping tokens -> integer ids. <UNK> tokens map to 0.\n",
        "\n",
        "        max_sequence_length(int): maximum length of a single text to use to create center-context pairs.\n",
        "            If length of sentence > max_sequence_length, truncate it to max_sequence_length.\n",
        "\n",
        "        window(int): size of the window to take as context for each middle word.\n",
        "            Example: [\n",
        "              \"Shalika likes to drink coffee in the morning.\"\n",
        "            ]\n",
        "            If window = 2, for middle word \"drink\", the pairs will be\n",
        "            [(\"drink\", \"to\"), (\"drink\", \"like\"), (\"drink\", \"coffee\"), (\"drink\", \"in\")]\n",
        "\n",
        "    Returns:\n",
        "        (torch.tensor, torch.tensor): Tensor of center word ids and Tensor of context word ids\n",
        "    \"\"\"\n",
        "\n",
        "    # BEGIN CODE : skipgram_init.collate_skipgram\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END CODE\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(split, batch_size, shuffle, min_freq, vocab=None):\n",
        "    \"\"\"\n",
        "    Get the dataloader and vocabulary for the given split.\n",
        "\n",
        "    Args:\n",
        "        split(str): 'train' or 'val'\n",
        "        batch_size(int): Batch size\n",
        "        shuffle(bool): Whether to shuffle the dataset\n",
        "        min_freq(int): Minimum frequency of words to include in vocabulary.\n",
        "            Words with frequency < min_freq are not included.\n",
        "        vocab(dict): Mapping of words to unique indices, including UNK token\n",
        "\n",
        "      Returns:\n",
        "        dataloader(torch.utils.data.DataLoader): Dataloader for the given split\n",
        "        vocab(dict): Mapping of words to unique indices, including UNK token\n",
        "    \"\"\"\n",
        "    texts = load_text(split)\n",
        "\n",
        "    if vocab is None:\n",
        "        vocab = create_vocabulary(texts, min_freq)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate_skipgram, vocab=vocab),\n",
        "    )\n",
        "\n",
        "    return dataloader, vocab\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You will have to implement the Forward Pass below.\n",
        "\n",
        "For each training example, we are given\n",
        "- a center word c\n",
        "- one true context word o\n",
        "- K negative words\n",
        "\n",
        "Let B = batch size and d = embedding dimension. In this forward function signature,\n",
        "\n",
        "- center_words has shape (B, )\n",
        "- context_words has shape (B, )\n",
        "- negative_words has shape (B, K, )\n",
        "\n",
        "The corresponding embedding tensors are,\n",
        "\n",
        "- $v_c$  = center_embeddings(center_words) with shape (B, d). $v_{c_i}$ denotes the embedding vector of the $i^{th}$ center word.\n",
        "- $u_o$  = context_embeddings(context_words) with shape (B, d). $u_{o_i}$ denotes the embedding vector of the true context word $o_i$ corresponding to the $i^{th}$ center word.\n",
        "- $u_{o'}$ = context_embeddings(negative_words) with shape (B, K, d).\n",
        "$u_{{o'}_{i,k}}$ denotes the embedding vector of the $k^{th}$ negative word corresponding to the $i^{th}$ center word.\n",
        "\n",
        "For the i-th example in the batch, the **positive score** is defined as:\n",
        "$$\n",
        "s_{pos}(i) = v_{c_i}^T u_{o_i}\n",
        "= \\sum_j v_c[i, j] ¬∑ u_o[i, j]\n",
        "$$\n",
        "\n",
        "The positive score is computed by taking an elementwise product\n",
        "between v_c and u_o and summing over the embedding dimension. Similarly the negative scores for the i-th example and k-th negative sample associated with it is defined as:\n",
        "$$\n",
        "s_{neg}(i, k) = v_{c_i}^T u_{o'_{i,k}}\n",
        "= \\sum_j v_c[i, j] \\cdot u_{o'}[i, k, j]\n",
        "$$\n",
        "\n",
        "The negative scores are computed by taking an elementwise product\n",
        "between v_c and every negative embedding u_o' and summing over the\n",
        "embedding dimension. Since each center word has K negative words, $v_c$ is reshaped to (B, 1, D) so that it can be multiplied with all K negative vectors\n",
        "for the same center word using broadcasting.\n",
        "\n",
        "The function returns pos_score with shape (B,) and neg_score with shape (B, K,).\n",
        "\n",
        "> Please note that the scores returned by the forward function in our implementation are NOT normalised scores, rather the raw dot products. Normalisation is performed in the `negative_sampling_loss` method that you will encounter in the next step."
      ],
      "metadata": {
        "id": "oHX4U4FQzC8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBcOyCI8pUXx"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class SkipGram_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Skip-Gram model described in paper:\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, embed_dimension: int = EMBED_DIMENSION):\n",
        "        super(SkipGram_Model, self).__init__()\n",
        "\n",
        "        # Embedding matrix for center words (v)\n",
        "        self.center_embeddings = nn.Embedding(vocab_size, embed_dimension)\n",
        "        # Embedding matrix for context words (u)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embed_dimension)\n",
        "\n",
        "\n",
        "    def forward(self, center_words, context_words, negative_words):\n",
        "        \"\"\"\n",
        "        Forward pass of the Skip-Gram model. This method computes scores measuring how well\n",
        "        a center word predicts surrounding context words. You will be using SkipGram training with\n",
        "        negative sampling in this implementation.\n",
        "\n",
        "        Negative Sampling computes similarity scores for:\n",
        "          (a) (center, context) word pairs\n",
        "          (b) a small set of randomly sampled negative words\n",
        "\n",
        "        Forward Training Steps:\n",
        "          - Get the embeddings for center words (v_c)\n",
        "          - Get the embeddings for positive (u_o) and negative context words (u_o')\n",
        "          - Get positive score as dot product of v_c and u_o\n",
        "          - Get negative score as the dot product of v_c and u_o'\n",
        "          - Score returned by the forward function is NOT normalised scores, rather the raw dot product.\n",
        "        Refer to the previous cell to get more information about the scores.\n",
        "\n",
        "        Args:\n",
        "          center_words (torch.tensor of shape (batch_size,)): The center word ids\n",
        "          context_words (torch.tensor of shape (batch_size, )): The context word ids\n",
        "          negative_words (torch.tensor of shape (batch_size, n_negatives)): Word ids of negative words corresponding the\n",
        "              the center word and context word in the same index.\n",
        "\n",
        "        Returns:\n",
        "          torch.tensor of shape (batch_size,): positive scores\n",
        "          torch.tensor of shape (batch_size,): negative scores\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : skipgram_model.forward\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VYrUOTE0Kst"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        optimizer,\n",
        "        lr_scheduler,\n",
        "        device,\n",
        "        model_dir,\n",
        "        model_name,\n",
        "        num_negatives=5,\n",
        "    ):\n",
        "\n",
        "        self.model = model  # Skipgram model\n",
        "        self.epochs = epochs # Number of epochs to train for\n",
        "        self.train_dataloader = train_dataloader # Dataloader object to load the training data\n",
        "        self.val_dataloader = val_dataloader # Dataloader object to load the validation data\n",
        "        self.optimizer = optimizer # Optimizer object\n",
        "        self.lr_scheduler = lr_scheduler # Learning rate scheduler\n",
        "        self.device = device # Device to use (CUDA/CPU)\n",
        "        self.model_dir = model_dir # Directory we save the model in\n",
        "        self.model_name = model_name # Name of the model\n",
        "        self.num_negatives = num_negatives # Number of negative samples per center word\n",
        "        self.loss = {\"train\": [], \"val\": []} # Save the training and validation loss values\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def negative_sampling_loss(self, pos_score, neg_score):\n",
        "        \"\"\"\n",
        "        Negative sampling loss function.\n",
        "\n",
        "        Recall that the negative sampling loss is given by\n",
        "        = - {sum[log(sigmoid(pos_score))] + sum[log(sigmoid(-neg_score))]}\n",
        "\n",
        "        Args:\n",
        "          pos_score (torch.tensor of shape (batch_size,)): positive scores\n",
        "          neg_score (torch.tensor of shape (batch_size, n_negatives)): negative scores\n",
        "        Returns:\n",
        "          torch.tensor of shape (1,): loss\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : skipgram_trainer.negative_sampling_loss\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def sample_negatives(self, context_words):\n",
        "        \"\"\"\n",
        "        Sample negatives uniformly, ensuring negatives are never equal to positive context words.\n",
        "\n",
        "        Args:\n",
        "          context_words (torch.Tensor): tensor of shape (batch_size,)\n",
        "        Returns:\n",
        "          torch.Tensor of shape (batch_size, num_negatives)\n",
        "        \"\"\"\n",
        "        batch_size = context_words.size(0)\n",
        "        vocab_size = self.model.context_embeddings.num_embeddings\n",
        "\n",
        "        # Initial uniform sampling\n",
        "        negatives = torch.randint(\n",
        "            low=0,\n",
        "            high=vocab_size,\n",
        "            size=(batch_size, self.num_negatives),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        # Ensure no negative equals the positive context word\n",
        "        context_words = context_words.unsqueeze(1)\n",
        "        mask = negatives == context_words\n",
        "\n",
        "        while mask.any():\n",
        "            resampled = torch.randint(\n",
        "                low=0,\n",
        "                high=vocab_size,\n",
        "                size=(mask.sum().item(),),\n",
        "                device=self.device\n",
        "            )\n",
        "            negatives[mask] = resampled\n",
        "            mask = negatives == context_words\n",
        "\n",
        "        return negatives\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Training function.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            self._train_epoch() # Training step\n",
        "            self._validate_epoch() # Validation step\n",
        "\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{self.epochs}] \"\n",
        "                f\"Train Loss: {self.loss['train'][-1]:.4f} | \"\n",
        "                f\"Val Loss: {self.loss['val'][-1]:.4f}\"\n",
        "            )\n",
        "\n",
        "            self.lr_scheduler.step() # Update the learning rate\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        \"\"\"\n",
        "        Training step for the model for one epoch.\n",
        "\n",
        "        Steps of training:\n",
        "        - Ensure that model is in training state\n",
        "        - Iterate through every batch of train_dataloader\n",
        "        - For each batch, get the model outputs, calculate loss using self.criterion\n",
        "          and update weights using backward()\n",
        "        - Update the optimizer (Ensure to call zero_grad() first)\n",
        "        - Update the average training loss across batches to self.loss\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for center, context in self.train_dataloader:\n",
        "            center = center.to(self.device)\n",
        "            context = context.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Sample negatives\n",
        "            negatives = self.sample_negatives(context)\n",
        "            # Get positive and negative similarities\n",
        "            pos_score, neg_score = self.model(center, context, negatives)\n",
        "            pos_score = torch.clamp(pos_score, min=-10, max=10)\n",
        "            neg_score = torch.clamp(neg_score, min=-10, max=10)\n",
        "            # Get the negative sampling loss\n",
        "            loss = self.negative_sampling_loss(pos_score, neg_score)\n",
        "            # Backpropagate\n",
        "            loss.backward()\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "        self.loss[\"train\"].append(np.mean(running_loss))\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        \"\"\"\n",
        "        Validation step for one epoch\n",
        "\n",
        "        Steps of validation:\n",
        "        - Ensure that model is in evaluation state\n",
        "        - Iterate through every batch of val_dataloader\n",
        "        - For each batch, get the model outputs and calculate loss using self.criterion.\n",
        "          Do not update weights or optimizer.\n",
        "        - Update the average validation loss across batches to self.loss\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for center, context in self.val_dataloader:\n",
        "                center = center.to(self.device)\n",
        "                context = context.to(self.device)\n",
        "\n",
        "                # Sample negatives\n",
        "                negatives = self.sample_negatives(context)\n",
        "                # Get positive and negative similarities\n",
        "                pos_score, neg_score = self.model(center, context, negatives)\n",
        "                pos_score = torch.clamp(pos_score, min=-10, max=10)\n",
        "                neg_score = torch.clamp(neg_score, min=-10, max=10)\n",
        "                # Get the negative sampling loss\n",
        "                loss = self.negative_sampling_loss(pos_score, neg_score)\n",
        "                # We do not backpropagate on the validation step\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "        self.loss[\"val\"].append(np.mean(running_loss))\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to directory\n",
        "        \"\"\"\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"\n",
        "        Save train/val loss as json file to the directory\n",
        "        \"\"\"\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAAp2RpIDTYW"
      },
      "outputs": [],
      "source": [
        "# You can choose and play around with training parameters here.\n",
        "# Note that we will be using our own set of training parameters\n",
        "# during evaluation, hence the values used here might not be indicative of\n",
        "# the final performance of your model.\n",
        "\n",
        "train_batch_size = None     # Number of training samples processed per gradient update\n",
        "val_batch_size = None       # Number of samples processed together during validation\n",
        "shuffle = True              # Randomly shuffle training data each epoch to improve learning\n",
        "learning_rate = None        # Step size for updating word embeddings during optimization\n",
        "num_negatives = None        # Number of negative samples to take per center word\n",
        "epochs = None               # Number of full passes over the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXlHJgi_DYPQ"
      },
      "outputs": [],
      "source": [
        "def get_lr_scheduler(optimizer, total_epochs: int):\n",
        "  \"\"\"\n",
        "  Learning rate scheduler to linearly decrease learning rate\n",
        "\n",
        "  Args:\n",
        "    optimizer: Optimizer\n",
        "    total_epochs: Total number of epochs\n",
        "  Returns:\n",
        "    lr_scheduler: Learning rate scheduler\n",
        "  \"\"\"\n",
        "  lr_lambda = lambda epoch: (total_epochs - epoch) / total_epochs\n",
        "  lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "  return lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWJWLXD2De-h"
      },
      "outputs": [],
      "source": [
        "# Load the training and validation data\n",
        "\n",
        "model_dir = \"skipgram_WikiText2\"\n",
        "model_name = 'skipgram'\n",
        "\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "train_dataloader, vocab = get_dataloader_and_vocab(\n",
        "    split=\"train\",\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    min_freq=MIN_WORD_FREQUENCY,\n",
        "    vocab=None,\n",
        ")\n",
        "\n",
        "val_dataloader, _ = get_dataloader_and_vocab(\n",
        "    split=\"validation\",\n",
        "    batch_size=val_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    min_freq=MIN_WORD_FREQUENCY,\n",
        "    vocab=vocab,\n",
        ")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiUZFiuADnm3"
      },
      "outputs": [],
      "source": [
        "# Training can take ~1 minute per epoch. Please be patient.\n",
        "# Ensure that the entire model training andevaluation does not exceed 15 minutes.\n",
        "\n",
        "model_class = SkipGram_Model\n",
        "model = model_class(vocab_size=vocab_size)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "lr_scheduler = get_lr_scheduler(optimizer, epochs)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    epochs=epochs,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=device,\n",
        "    model_dir=model_dir,\n",
        "    model_name=model_name,\n",
        "    num_negatives=num_negatives\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.save_loss()\n",
        "vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "torch.save(vocab, vocab_path)\n",
        "print(\"Model artifacts saved to folder:\", model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging Help:\n",
        "\n",
        "\n",
        "> If you encounter ```CUDA OUT OF MEMORY``` error while training the SkipGram model, reduce the batch size and/or embedding dimension and restart session.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eznlzl6EFsZd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcgRxigtqbHu"
      },
      "source": [
        "## 2.2 Evaluating the goodness of the trained model embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N85zYM-GEexp"
      },
      "source": [
        "Timothy reads about a mechanism that helps machines decide whether two things are close in meaning or far apart: **similarity functions**. When words, documents, or images are represented as vectors, similarity functions act like rulers in vector space. They tell us how alike two vectors are.\n",
        "\n",
        "Two of the most commonly used similarity measures are Cosine Similarity and Euclidean Distance.\n",
        "\n",
        "### 1. Cosine Similarity\n",
        "\n",
        "Cosine similarity measures the **angle** between two vectors, not their magnitude. It answers the question: *Are these vectors pointing in the same direction?*\n",
        "\n",
        "$\\text{CosineSimilarity}(u, v) = \\frac{u \\cdot v}{|u| \\cdot |v|}$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $ u \\cdot v $ is the dot product of the vectors\n",
        "* |u| and |v| are their magnitudes (L2 norms)\n",
        "* Function Range: **[-1, 1]**\n",
        "\n",
        "Cosine similarity is especially popular in **NLP** because it ignores vector length and focuses purely on meaning\n",
        "\n",
        "### 2. Euclidean Distance\n",
        "\n",
        "Euclidean distance measures the **straight-line distance** between two points in space. It answers the question: *How far apart are these vectors?*\n",
        "\n",
        "\n",
        "$\\text{EuclideanDistance}(u, v) = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2}$\n",
        "\n",
        "where\n",
        "\n",
        "* $u$ and $v$ are vectors of dimension $n$\n",
        "* Function Range: **[0, ‚àû)**\n",
        "\n",
        "\n",
        "Euclidean distance is sensitive to **magnitude**, making it useful when vector length itself carries meaning\n",
        "\n",
        "Timothy asks you to use this newfound knowledge to determine if the embeddings created by your model are actually any good.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class WordEmbeddingOps:\n",
        "    def __init__(self, word2vec_path, vocab_path=None, trained_skipgram_model=True):\n",
        "        \"\"\"\n",
        "        Initialize the WordEmbeddings class with a pre-trained word2vec model.\n",
        "\n",
        "        Args:\n",
        "            word2vec_path (str): Path to the word2vec model file\n",
        "                Example: 'path/to/skipgram_model.pt'\n",
        "            vocab_path (str): Path to the vocabulary file\n",
        "                Example: 'path/to/vocab.txt'\n",
        "\n",
        "        Note:\n",
        "            - Loads word vectors using gensim's KeyedVectors\n",
        "        \"\"\"\n",
        "        if trained_skipgram_model:\n",
        "            # Load your trained word embedding model\n",
        "            self.model = torch.load(word2vec_path, weights_only=False)\n",
        "            self.embeddings = list(self.model.parameters())[0].cpu().detach().numpy()\n",
        "            self.vocab = torch.load(vocab_path, weights_only=False)\n",
        "        else:\n",
        "            # Download off-the-shelf word2vec from keyed vectors\n",
        "            word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "            self.embeddings = word2vec.vectors\n",
        "            self.vocab = {w: i for i, w in enumerate(word2vec.index_to_key)}\n",
        "\n",
        "        self.norms = np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
        "        self.embeddings_norm = self.embeddings / self.norms\n",
        "        self.idx2word = {idx: word for word, idx in self.vocab.items()}\n",
        "\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        \"\"\"\n",
        "        Complete this function to calculate cosine similarity between two vectors.\n",
        "        Recall that cosine similarity = vec1 ¬∑ vec2 / (||vec1|| ||vec2||)\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.array): First vector\n",
        "                Example: array([0.2, 0.5, -0.1])\n",
        "            vec2 (np.array): Second vector\n",
        "                Example: array([0.3, 0.4, -0.2])\n",
        "\n",
        "        Returns:\n",
        "            float: Cosine similarity between vectors\n",
        "                Example: 0.95 (for above vectors)\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.cosine_similarity\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def euclidean_similarity(self, vec1, vec2):\n",
        "        \"\"\"\n",
        "        Complete this function to calculate similarity based on Euclidean distance.\n",
        "        Recall that Euclidean similarity = 1 / (1 + Euclidean distance)\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.array): First vector\n",
        "                Example: array([0.2, 0.5, -0.1])\n",
        "            vec2 (np.array): Second vector\n",
        "                Example: array([0.3, 0.4, -0.2])\n",
        "\n",
        "        Returns:\n",
        "            float: Similarity score based on Euclidean distance\n",
        "                Example: 0.85\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.euclidean_similarity\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def get_word_similarity(self, words, similarity_func='cosine'):\n",
        "        \"\"\"\n",
        "        This function takes a list of words and computes the pairwise similarity between every pair of words\n",
        "        The word vectors are returned as a NumPy array so that dimensionality\n",
        "        reduction techniques such as PCA can be applied later.\n",
        "\n",
        "        Args:\n",
        "            words (list): List of words for which embeddings and similarities\n",
        "                          need to be computed.\n",
        "            embeddings (np.ndarray): Word2Vec embedding matrix of shape\n",
        "                                    (vocab_size, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            vectors (np.ndarray): Word embeddings corresponding to the input words.\n",
        "                                  Shape: (len(words), embedding_dim)\n",
        "            w2v_similarity (np.ndarray): Pairwise cosine similarity matrix.\n",
        "                                        Shape: (len(words), len(words))\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        indices = [self.vocab.get(w, self.vocab.get(\"<UNK>\", None)) for w in words]\n",
        "        indices = [idx for idx in indices if idx is not None]\n",
        "        vectors = self.embeddings[indices]\n",
        "\n",
        "        n = len(words)\n",
        "        sim_matrix = np.zeros((n, n))\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if similarity_func == 'cosine':\n",
        "                    sim_matrix[i, j] = self.cosine_similarity(vectors[i], vectors[j])\n",
        "                else:\n",
        "                    sim_matrix[i, j] = self.euclidean_similarity(vectors[i], vectors[j])\n",
        "\n",
        "        return vectors, sim_matrix\n",
        "\n",
        "    def find_similar_words(self, word, num_results=5, similarity_func='cosine'):\n",
        "        \"\"\"\n",
        "        Complete this function to find the most similar words to a given word.\n",
        "\n",
        "        Implementation:\n",
        "          - Iterate through entire vocabulary using:\n",
        "            for word in self.word_vectors.index_to_key\n",
        "            This is necessary to find the most similar words by comparing the target word's vector\n",
        "            against all known words\n",
        "          - Exclude input words from results.\n",
        "          - Return empty list if any input word not in vocabulary.\n",
        "\n",
        "        Args:\n",
        "            word (str): Input word to find similar words for\n",
        "                Example: 'computer'\n",
        "            num_results (int): Number of similar words to return\n",
        "                Example: 5\n",
        "            similarity_func (str): Similarity function to use ('cosine' or 'euclidean')\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (word, similarity_score) for top num_results matches\n",
        "                Example: [('laptop', 0.89), ('pc', 0.87), ('desktop', 0.85), ...\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.find_similar_words\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ],
      "metadata": {
        "id": "OwcXoGvgtamF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You start by checking how the similarities between embeddings of different words look like using the trained SkipGram model."
      ],
      "metadata": {
        "id": "YEJEV7fA-4hm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q_CUMI5qpNM"
      },
      "outputs": [],
      "source": [
        "words = ['king', 'queen',\n",
        "         'television', 'computer',\n",
        "         'river', 'water']\n",
        "\n",
        "# Load the trained model\n",
        "skipgram_model_path = os.path.join(model_dir, \"model.pt\")\n",
        "vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "\n",
        "# Create embeddings\n",
        "word_embedding_ops = WordEmbeddingOps(skipgram_model_path, vocab_path)\n",
        "\n",
        "# Find similarities between all pairs of words using COSINE SIMILARITY\n",
        "w2v_vectors, w2v_similarity = word_embedding_ops.get_word_similarity(words, similarity_func='cosine')\n",
        "sim = w2v_similarity.copy()\n",
        "np.fill_diagonal(sim, np.nan)\n",
        "vmin = np.nanmin(sim)\n",
        "vmax = np.nanmax(sim)\n",
        "\n",
        "# Visualise on a heatmap\n",
        "sns.heatmap(\n",
        "    pd.DataFrame(sim, columns=words, index=words),\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap=\"viridis\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similarities between all pairs of words using EUCLIDEAN SIMILARITY\n",
        "w2v_vectors, w2v_similarity = word_embedding_ops.get_word_similarity(words, similarity_func='euclidean')\n",
        "sim = w2v_similarity.copy()\n",
        "np.fill_diagonal(sim, np.nan)\n",
        "vmin = np.nanmin(sim)\n",
        "vmax = np.nanmax(sim)\n",
        "\n",
        "# Visualise on a heatmap\n",
        "sns.heatmap(\n",
        "    pd.DataFrame(sim, columns=words, index=words),\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap=\"viridis\"\n",
        ")"
      ],
      "metadata": {
        "id": "5XwK7DWeDn-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you are satisfied with the fact that embeddings do carry some meaning, you try to deep dive into how the similarity between related and unrelated words relate"
      ],
      "metadata": {
        "id": "9sZj_-rFD63-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_related_vs_unrelated_words(word_embedding_ops):\n",
        "    word_relationships = {\n",
        "        'king': {\n",
        "            'related': ['queen', 'prince', 'royal', 'throne', 'kingdom'],\n",
        "            'unrelated': ['computer', 'ocean', 'bicycle', 'coffee', 'mountain']\n",
        "        },\n",
        "        'ocean': {\n",
        "            'related': ['sea', 'wave', 'coast', 'marine', 'island'],\n",
        "            'unrelated': ['book', 'war', 'music', 'building', 'science']\n",
        "        },\n",
        "        'year': {\n",
        "            'related': ['month', 'day', 'time', 'period', 'decade'],\n",
        "            'unrelated': ['ocean', 'food', 'building', 'bicycle', 'coffee']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    related_scores = []\n",
        "    unrelated_scores = []\n",
        "\n",
        "    for word, relations in word_relationships.items():\n",
        "        if word not in word_embedding_ops.vocab:\n",
        "            continue\n",
        "\n",
        "        print(f\"Target word = '{word}'\")\n",
        "        word_vec = word_embedding_ops.embeddings[word_embedding_ops.vocab[word]]\n",
        "\n",
        "        print(f\"Similarities with related words:\")\n",
        "        word_related_scores = []\n",
        "        for rel in relations['related']:\n",
        "            if rel in word_embedding_ops.vocab:\n",
        "                rel_vec = word_embedding_ops.embeddings[word_embedding_ops.vocab[rel]]\n",
        "                sim = word_embedding_ops.cosine_similarity(word_vec, rel_vec)\n",
        "                related_scores.append(sim)\n",
        "                word_related_scores.append(sim)\n",
        "                print(f\"{word} and {rel} - {sim:.3f}\")\n",
        "\n",
        "        print(f\"Similarities with unrelated words:\")\n",
        "        word_unrelated_scores = []\n",
        "        for unrel in relations['unrelated']:\n",
        "            if unrel in word_embedding_ops.vocab:\n",
        "                unrel_vec = word_embedding_ops.embeddings[word_embedding_ops.vocab[unrel]]\n",
        "                sim = word_embedding_ops.cosine_similarity(word_vec, unrel_vec)\n",
        "                unrelated_scores.append(sim)\n",
        "                word_unrelated_scores.append(sim)\n",
        "                print(f\"{word} and {unrel} - {sim:.3f}\")\n",
        "\n",
        "        if word_related_scores and word_unrelated_scores:\n",
        "            avg_rel = np.mean(word_related_scores)\n",
        "            avg_unrel = np.mean(word_unrelated_scores)\n",
        "            diff = avg_rel - avg_unrel\n",
        "            print(f\"\\'{word}' summary:\")\n",
        "            print(f\"Avg related: {avg_rel:.3f}\")\n",
        "            print(f\"Avg unrelated: {avg_unrel:.3f}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "# The average similarity between related words should be higher than that of unrelated words.\n",
        "# This indicates that the embeddings carry meaning.\n",
        "experiment_results = experiment_related_vs_unrelated_words(word_embedding_ops)"
      ],
      "metadata": {
        "id": "BMEi-JQb3-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timothy is happy with your findings. But like all good guides, he has suggestions. Instead of matching similarities between already selected words, Timothy wants you to explore more openly to see if the embeddings carry any meaning. He wants you to take a words and find the words closest in the embeddings space to it, to verify if they are indeed related words."
      ],
      "metadata": {
        "id": "4aiyEzP6FgEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding_ops.find_similar_words('king')"
      ],
      "metadata": {
        "id": "fxx9dYkDFNgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding_ops.find_similar_words('market')"
      ],
      "metadata": {
        "id": "QkZnC6lyF3zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding_ops.find_similar_words('fast')"
      ],
      "metadata": {
        "id": "xVjGEYDoWajV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donot be disappointed if you do not see synonyms in the results here. This is a miniature embedding model and hence word relationships are not very well baked in it.\n",
        "\n",
        "Timothy is aware of the problems of training a SkipGram model from scratch. And to his delight he discovers the ```word2vec-google-news-300``` pretrained Word2Vec model that generated 300d embeddings and is pretrained on a huge corpus, thus gives much more meaningful word embeddings. This finding is going to help you in the next segment of this assignment."
      ],
      "metadata": {
        "id": "JDk6aLW0HDFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Evaluating the goodness of `word2vec-google-news-300` embeddings"
      ],
      "metadata": {
        "id": "tQRjO3n1UDrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_word2vec_model(model_name=\"word2vec-google-news-300\"):\n",
        "    \"\"\"\n",
        "    Download word2vec model using gensim's built-in downloader.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to download.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded model\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the specified model is not available\n",
        "        Exception: For other download or processing errors\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if model is available\n",
        "        available_models = api.info()['models'].keys()\n",
        "        if model_name not in available_models:\n",
        "            raise ValueError(\n",
        "                f\"Model '{model_name}' not found. Available models: {', '.join(available_models)}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Downloading {model_name}...\")\n",
        "        model_path = api.load(model_name, return_path=True)\n",
        "        print(f\"Model downloaded successfully to: {model_path}\")\n",
        "\n",
        "        return model_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# This might take up to five minutes in the first run, so please be patient!\n",
        "word2vec_path = download_word2vec_model()"
      ],
      "metadata": {
        "id": "-i3lOs-3XqRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['king', 'queen',\n",
        "         'television', 'computer',\n",
        "         'river', 'water']\n",
        "\n",
        "# Create embeddings\n",
        "word2vec_word_embedding_ops = WordEmbeddingOps(word2vec_path, vocab_path=None, trained_skipgram_model=False)\n",
        "\n",
        "# Find similarities between all pairs of words using COSINE SIMILARITY\n",
        "w2v_vectors, w2v_similarity = word2vec_word_embedding_ops.get_word_similarity(words, similarity_func='cosine')\n",
        "sim = w2v_similarity.copy()\n",
        "np.fill_diagonal(sim, np.nan)\n",
        "vmin = np.nanmin(sim)\n",
        "vmax = np.nanmax(sim)\n",
        "\n",
        "# Visualise on a heatmap\n",
        "sns.heatmap(\n",
        "    pd.DataFrame(sim, columns=words, index=words),\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap=\"viridis\"\n",
        ")"
      ],
      "metadata": {
        "id": "U0gOZ_2F0r2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similarities between all pairs of words using EUCLIDEAN SIMILARITY\n",
        "w2v_vectors, w2v_similarity = word2vec_word_embedding_ops.get_word_similarity(words, similarity_func='euclidean')\n",
        "sim = w2v_similarity.copy()\n",
        "np.fill_diagonal(sim, np.nan)\n",
        "vmin = np.nanmin(sim)\n",
        "vmax = np.nanmax(sim)\n",
        "\n",
        "# Visualise on a heatmap\n",
        "sns.heatmap(\n",
        "    pd.DataFrame(sim, columns=words, index=words),\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap=\"viridis\"\n",
        ")"
      ],
      "metadata": {
        "id": "FJnRikY1a4Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The average similarity between related words should be higher than that of unrelated words.\n",
        "# This indicates that the embeddings carry meaning.\n",
        "experiment_results = experiment_related_vs_unrelated_words(word2vec_word_embedding_ops)"
      ],
      "metadata": {
        "id": "Mj4EvIpH08Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_word_embedding_ops.find_similar_words('king')"
      ],
      "metadata": {
        "id": "Qzpspo0AUlkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_word_embedding_ops.find_similar_words('market')"
      ],
      "metadata": {
        "id": "oIWec_aPbT08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_word_embedding_ops.find_similar_words('fast')"
      ],
      "metadata": {
        "id": "oZCATuQObUHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained embeddings are thus able to encode semantic information about words and are hence essential tools in understanding the semantics of natural language text. With this motivation, you move on to the final part of this assignment."
      ],
      "metadata": {
        "id": "cD6e2OrBb1Nq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO3DyVF01k9p"
      },
      "source": [
        "# 3. Discriminative Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVTTjgwanu-"
      },
      "source": [
        "After spending days multiplying probabilities, Timothy stumbles upon a different philosophy of classification. Instead of asking *how* a document is generated, this approach asks a simpler, sharper question:\n",
        "\n",
        "> *Given a document, which class does it belong to?*\n",
        "\n",
        "This family of methods is called **discriminative classification**.\n",
        "\n",
        "Let:\n",
        "\n",
        "- Let $d = (w_1, w_2, \\dots, w_n)$ be a document or a piece of text with multiple words\n",
        "- Let $c$ be a class (Politics, Sports, Technology, etc.)\n",
        "\n",
        "Unlike generative models, which learn the $ P(d \\mid c) $, discriminative models focus directly on learning $ P(c \\mid d) $,\n",
        "or equivalently, a **decision boundary** that separates classes in a feature space.\n",
        "\n",
        "Your task now is to convert documents into numerical representations and learn a function\n",
        "$\n",
        "f(d) \\rightarrow c\n",
        "$\n",
        "that assigns each document to one of the news categories.\n",
        "\n",
        "\n",
        "## Word Representations\n",
        "\n",
        "To construct decision boundaries, documents must first be represented as vectors. Timothy explores two popular representations.\n",
        "\n",
        "### (a) Bag-of-Words (BoW)\n",
        "\n",
        "In the Bag-of-Words representation, a document is treated as an **unordered collection of words**, ignoring grammar and word order.\n",
        "\n",
        "Let the vocabulary be\n",
        "$$\n",
        "V = {w_1, w_2, \\dots, w_{|V|}}.\n",
        "$$\n",
        "\n",
        "A document (d) is represented as a vector\n",
        "$$\n",
        "x_d \\in \\mathbb{R}^{|V|}\n",
        "$$\n",
        "\n",
        "where each dimension corresponds to a word in the vocabulary. The $i$-th component is defined as:\n",
        "$$\n",
        "(x_d)_i = \\text{count}(w_i \\in d)\n",
        "$$\n",
        "\n",
        "For example, if our vocabulary looks like {0: <UNK>, 1: tea, 2: coffee, 3: hot, 4: cold, 5: is, 6: the, 7:not, 8:enough}, then the sentence \"The hot coffee is not hot enough\" will be represented as $[0, 0, 1, 2, 0, 1, 1, 1, 1]$\n",
        "\n",
        "Using this representation, we can train a discriminative classifier that learns a weight matrix $W$ and bias $b$, computing class scores as:\n",
        "$$\n",
        "\\mathbf{s} = W \\cdot \\mathbf{x}_d + b.\n",
        "$$\n",
        "\n",
        "The predicted class is:\n",
        "$$\n",
        "c_{pred} = \\arg\\max_c \\space s_c.\n",
        "$$\n",
        "\n",
        "While simple and effective, BoW representations suffers from two major problems,\n",
        "\n",
        "- High dimensionality - The size of the vector representation increases with increasing size of vocabularies. This makes BOW representations non-scalable.\n",
        "\n",
        "- Lack of semantic understanding - Words like *‚Äúelection‚Äù* and *‚Äúvote‚Äù* will be treated as completely unrelated in this scheme, while they should ideally be treated somewhat similarly.\n",
        "\n",
        "### (b) Word2Vec-Based Representation\n",
        "\n",
        "To overcome the limitations of BoW, Timothy realises that you can use the **Word2Vec embeddings** trained earlier. But you also saw that the trained embeddings were not the best. So you suggested that you use the pretrained ```word2vec-google-news-300``` embedddings and train a simple softmax classifier.\n",
        "\n",
        "Each word $ w \\in V $ is mapped to a dense vector:\n",
        "$$\n",
        "e_w \\in \\mathbb{R}^{d},\n",
        "$$\n",
        "\n",
        "A document is represented by aggregating the embeddings of its words. A common approach is mean pooling:\n",
        "\n",
        "$$\n",
        "x_d = \\frac{1}{n} \\sum_{i=1}^{n} (e_w)_i,\n",
        "$$\n",
        "\n",
        "where $ d = (w_1, w_2, \\dots, w_n) $. Here $ d \\ll |V|$. Hence, this produces a dense, low-dimensional representation that captures semantic information.\n",
        "\n",
        "The same discriminative classifier is then applied:\n",
        "$$\n",
        "s = W \\cdot x_d + b,\n",
        "\\qquad\n",
        "c_{pred} = \\arg\\max_c \\space s_c.\n",
        "$$\n",
        "\n",
        "Unlike Bag-of-Words, Word2Vec-based representations allow documents with similar meanings to lie close together in the feature space, resulting in smoother and more meaningful decision boundaries.\n",
        "\n",
        "\n",
        "Timothy hands you these tools and you are finally ready to draw clean lines between the different news categories using Discriminative Classification.\n",
        "\n",
        "\n",
        "**Note:**\n",
        "Donot worry about the training code below, you are only expected to implement the topics discussed in our lecture and the rest is already provided to you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgSqqR-Yanu_"
      },
      "source": [
        "\n",
        "## 3.1 Bag-of-Words Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNSjctmvAJKn"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Define the softmax function\n",
        "    The softmax function takes as input a vector z\n",
        "    and outputs a vector of the same shape,\n",
        "    where each element is between 0 and 1\n",
        "\n",
        "    Args:\n",
        "        z (np.array): Input array\n",
        "    Returns:\n",
        "        np.array: Softmax of the input array\n",
        "    \"\"\"\n",
        "    # BEGIN CODE : softmax\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END CODE\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXOAqL8PanvA"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class BagOfWordsClassifier:\n",
        "    def __init__(self, model_dir, min_freq=1):\n",
        "        \"\"\"\n",
        "        Initialize the Bag of Words classifier.\n",
        "\n",
        "        Args:\n",
        "            model_dir (str): Directory to save/load trained classifier parameters.\n",
        "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
        "                           Words appearing less than min_freq times will be treated as UNK token.\n",
        "                           Default: 1 (include all words)\n",
        "\n",
        "        Attributes:\n",
        "            vocabulary (dict): Word to index mapping, including special UNK token\n",
        "                Example: {'<UNK>': 0, 'good': 1, 'movie': 2}\n",
        "            W: The weight matrix\n",
        "            b: The bias vector\n",
        "            lr (float): Learning rate for gradient descent\n",
        "            epochs (int): Number of training epochs\n",
        "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
        "                Example: If min_freq=2, words must appear at least twice to be included\n",
        "            model_dir (string): Directory to save/load trained classifier parameters.\n",
        "        \"\"\"\n",
        "        self.vocabulary = None\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "        self.min_freq = min_freq\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "\n",
        "    def text_to_bow(self, texts):\n",
        "        \"\"\"\n",
        "        Convert texts to bag-of-words feature vectors using the vocabulary,\n",
        "        where each element represents the count of occurrences of the word in the text.\n",
        "        Words not in vocabulary are mapped to UNK token and the first column should represent UNK tokens.\n",
        "        Use preprocess_text function for preprocessing the texts.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"government passes new law\", \"government announces new policy\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Document-term matrix with UNK handling\n",
        "                      Shape of output should be (n_documents, len(vocabulary))\n",
        "\n",
        "            Example: For vocabulary {'<UNK>':0, 'government':1, 'new':2, 'announces':3} with min_freq=2:\n",
        "                array([\n",
        "                      [1, 1, 1, 0],  # First doc: 1 UNK ('passes'), 1 'government', 1 'new', 0 'announces'\n",
        "                      [0, 1, 1, 1]   # Second doc: 0 UNKs, 1 'government', 1 'new', 1 'announces'\n",
        "                ])\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : bow.text_to_bow\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def fit(self, train_df, lr=0.1, epochs=40):\n",
        "        \"\"\"\n",
        "        Train the classifier on text documents.\n",
        "\n",
        "        Args:\n",
        "            train_df (pd.DataFrame): Must contain columns [\"text\", \"label\"]\n",
        "            lr (float): Learning rate for gradient descent\n",
        "            epochs (int): Number of training epochs\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract features and labels\n",
        "        X = train_df[\"text\"].values\n",
        "        y = train_df[\"label\"].values\n",
        "\n",
        "        # Split into training and validation sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Create vocabulary from training texts\n",
        "        self.vocabulary = create_vocabulary(X_train, min_freq=self.min_freq)\n",
        "\n",
        "        # Convert texts to BoW features\n",
        "        X_train_bow = self.text_to_bow(X_train)\n",
        "        X_val_bow = self.text_to_bow(X_val)\n",
        "\n",
        "        # Get shapes\n",
        "        n_samples, n_features = X_train_bow.shape\n",
        "        n_classes = len(np.unique(y_train))\n",
        "\n",
        "        # One-hot encode labels\n",
        "        y_train_onehot = np.zeros((n_samples, n_classes))\n",
        "        y_train_onehot[np.arange(n_samples), y_train] = 1\n",
        "\n",
        "        n_val_samples = X_val_bow.shape[0]\n",
        "        y_val_onehot = np.zeros((n_val_samples, n_classes))\n",
        "        y_val_onehot[np.arange(n_val_samples), y_val] = 1\n",
        "\n",
        "        # Initialise weights and bias\n",
        "        self.W = np.zeros((n_features, n_classes))\n",
        "        self.b = np.zeros((1, n_classes))\n",
        "\n",
        "        # Training loop\n",
        "        for ep in range(1, epochs + 1):\n",
        "            # Forward pass\n",
        "            logits = X_train_bow @ self.W + self.b\n",
        "            probs = softmax(logits)\n",
        "\n",
        "            # Gradient computation\n",
        "            error = probs - y_train_onehot\n",
        "            dW = (X_train_bow.T @ error) / n_samples\n",
        "            db = np.mean(error, axis=0, keepdims=True)\n",
        "\n",
        "            # Update parameters\n",
        "            self.W -= lr * dW\n",
        "            self.b -= lr * db\n",
        "\n",
        "            # Validation in every 5 epochs\n",
        "            if ep % 10 == 0:\n",
        "                train_loss = -np.sum(y_train_onehot * np.log(probs + 1e-15)) / n_samples\n",
        "                val_logits = X_val_bow @ self.W + self.b\n",
        "                val_probs = softmax(val_logits)\n",
        "                val_loss = -np.sum(y_val_onehot * np.log(val_probs + 1e-15)) / n_val_samples\n",
        "                print(f\"Epoch {ep:3d} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"Kohli scored a century\", \"Nvidia launches NVLink 6\",\n",
        "                          \"US President resigns\"]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [1, 0, 3] # 0(Tech) 1(Sports) 2(Business) 3(Politics)\n",
        "        \"\"\"\n",
        "        # Convert texts to BoW features\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "\n",
        "        # Make predictions\n",
        "        logits = X_bow @ self.W + self.b\n",
        "        probs = softmax(logits)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def get_class_probabilities(self, X_text):\n",
        "        \"\"\"\n",
        "        Calculate prediction confidence scores for each class. In other words,\n",
        "        return the probability distribution over classes.\n",
        "\n",
        "        Sanity Check:\n",
        "          - Each row sums to 1.0\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"Kohli scored a century\", \"Nvidia launches NVLink 6\",\n",
        "                          \"US President resigns\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Confidence scores for each class (values 0-1)\n",
        "                Example: array([0.1, 0.9, 0.12, 0.01])  # 90% confidence for Sports class\n",
        "        \"\"\"\n",
        "\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "        logits = X_bow @ self.W + self.b\n",
        "        return self.softmax(logits)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to directory\n",
        "        \"\"\"\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "        save_path = os.path.join(self.model_dir, \"bow_classifier.pkl\")\n",
        "        state = {\n",
        "            \"W\": self.W,\n",
        "            \"b\": self.b\n",
        "        }\n",
        "        joblib.dump(state, save_path)\n",
        "        print(f\"Model saved to {save_path}\")\n",
        "\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QTi-pe6x8vG"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate classification accuracy.\n",
        "    Accuracy = (number of correct predictions) / (total number of predictions)\n",
        "\n",
        "    Args:\n",
        "        y_true (list): True class labels\n",
        "            Example: [0, 1, 3, 1]\n",
        "        y_pred (list): Predicted class labels\n",
        "            Example: [0, 2, 3, 1]\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy (proportion of correct predictions)\n",
        "            Example: 0.75 (3 correct predictions out of 4)\n",
        "    \"\"\"\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"Length of true and predicted labels must match\")\n",
        "\n",
        "    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
        "    return correct / len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNiKmJbNDXMk"
      },
      "outputs": [],
      "source": [
        "# You can play with the values of the hyperparameters and notice how the accuracy\n",
        "# changes accordingly.\n",
        "\n",
        "MIN_FREQ = False # Minimum number of times a word has to be encountered to add it to the vocabulary\n",
        "LEARNING_RATE = False # Learning rate\n",
        "NUM_EPOCHS = False # Number of epochs to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5Dw9qW8DBOy"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate BoW classifier\n",
        "print(\"Training Bag of Words classifier...\")\n",
        "model_dir = f'weights/bow_classifier'\n",
        "bow_clf = BagOfWordsClassifier(model_dir, min_freq = MIN_FREQ)\n",
        "bow_clf.fit(train_df, lr = LEARNING_RATE, epochs = NUM_EPOCHS)\n",
        "bow_clf.save_model()\n",
        "\n",
        "X_test = test_df[\"text\"].values\n",
        "y_test = test_df[\"label\"].values\n",
        "\n",
        "bow_predictions = bow_clf.predict(X_test)\n",
        "bow_accuracy = evaluate_classifier(y_test, bow_predictions)\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "print(f\"BoW Validation Accuracy: {bow_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBgZZA8opWzx"
      },
      "source": [
        "## 3.2 Word2Vec-based Classifier\n",
        "\n",
        "Here, Timothy averages word embeddings or uses pooling:\n",
        "\n",
        "$$\n",
        "\\mathbf{d} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{v}_{w_i}\n",
        "$$\n",
        "\n",
        "This dense representation feeds into a classifier.\n",
        "\n",
        "The question Timothy wants answered:\n",
        "\n",
        "> Does meaning beat counting?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the pre-trained word2vec model, previously downloaded for skipgram assessment"
      ],
      "metadata": {
        "id": "Wyqb2O6N1G7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_word2vec_model(model_name=\"word2vec-google-news-300\"):\n",
        "    \"\"\"\n",
        "    Download word2vec model using gensim's built-in downloader.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to download.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded model\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the specified model is not available\n",
        "        Exception: For other download or processing errors\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if model is available\n",
        "        available_models = api.info()['models'].keys()\n",
        "        if model_name not in available_models:\n",
        "            raise ValueError(\n",
        "                f\"Model '{model_name}' not found. Available models: {', '.join(available_models)}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Downloading {model_name}...\")\n",
        "        model_path = api.load(model_name, return_path=True)\n",
        "        print(f\"Model downloaded successfully to: {model_path}\")\n",
        "\n",
        "        return model_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# This might take up to five minutes in the first run, so please be patient!\n",
        "word2vec_path = download_word2vec_model()"
      ],
      "metadata": {
        "id": "fjGErthtPULD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFTLcQtvwqtA"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class Word2VecClassifier:\n",
        "    def __init__(self, word2vec_path, model_dir):\n",
        "        \"\"\"\n",
        "        Word2Vec-based classifier (multiclass softmax):\n",
        "\n",
        "        Workflow:\n",
        "        1. Load a pre-trained Word2Vec model.\n",
        "        2. Represent each document as the average of its Word2Vec embeddings.\n",
        "        3. Train a linear softmax classifier (W, b) on top of these averaged vectors\n",
        "           for multiclass classification using gradient descent.\n",
        "\n",
        "        Attributes:\n",
        "            word_vectors (KeyedVectors): Loaded Word2Vec embeddings for all words in the vocabulary.\n",
        "            W (np.array or torch.Tensor): Weight matrix of shape (embedding_dim, n_classes), initialized later.\n",
        "            b (np.array or torch.Tensor): Bias vector of shape (n_classes,), initialized later.\n",
        "            model_dir (str): Directory to save/load trained classifier parameters.\n",
        "        \"\"\"\n",
        "        # Load the Word2Vec model\n",
        "        self.word_vectors = KeyedVectors.load_word2vec_format(\n",
        "            word2vec_path, binary=True)\n",
        "\n",
        "        # Define W and b for the multiclass classifier\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "        # Directory to save the model\n",
        "        self.model_dir = model_dir\n",
        "\n",
        "    def text_to_w2v(self, texts):\n",
        "        \"\"\"\n",
        "        Convert list of texts into document vectors by averaging word vectors\n",
        "        from the Word2Vec embeddings.\n",
        "\n",
        "        For each text document:\n",
        "          - Preprocess using ``preprocess_text`` to get tokens\n",
        "          - Get vector embedding corresponding to each token inthevocabulary\n",
        "          - Average the embeddings to get a document vector.\n",
        "          - If no word is present in the vocabulary, return a zero vector.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"Kohli scored a century\", \"Nvidia launches NVLink 6\",\n",
        "                          \"US President resigns\"]\n",
        "        Returns:\n",
        "            np.array: Document vectors with shape (n_documents, EMBED_DIMENSION)\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : w2v.text_to_w2v\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def fit(self, train_df, lr=0.1, epochs=40):\n",
        "        \"\"\"\n",
        "        Train the classifier on the provided training dataframe.\n",
        "\n",
        "        Args:\n",
        "            train_df (pd.DataFrame): Must contain columns [\"text\", \"label\"]\n",
        "            lr (float): Learning rate for gradient descent\n",
        "            epochs (int): Number of training epochs\n",
        "        \"\"\"\n",
        "        X = train_df[\"text\"].values\n",
        "        y = train_df[\"label\"].values\n",
        "\n",
        "        # Split into training and validation sets (same as BoW)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Convert texts to averaged word2vec vector\n",
        "        X_train_vec = self.text_to_w2v(X_train)\n",
        "        X_val_vec = self.text_to_w2v(X_val)\n",
        "\n",
        "        X_train_vec /= np.linalg.norm(X_train_vec, axis=1, keepdims=True) + 1e-8\n",
        "        X_val_vec   /= np.linalg.norm(X_val_vec, axis=1, keepdims=True) + 1e-8\n",
        "\n",
        "        n_samples, n_features = X_train_vec.shape\n",
        "        n_classes = len(np.unique(y_train))\n",
        "\n",
        "        # One-hot encode labels\n",
        "        y_train_onehot = np.zeros((n_samples, n_classes))\n",
        "        y_train_onehot[np.arange(n_samples), y_train] = 1\n",
        "\n",
        "        n_val_samples = X_val_vec.shape[0]\n",
        "        y_val_onehot = np.zeros((n_val_samples, n_classes))\n",
        "        y_val_onehot[np.arange(n_val_samples), y_val] = 1\n",
        "\n",
        "        # Initialise weights and bias\n",
        "        self.W = np.zeros((n_features, n_classes))\n",
        "        self.b = np.zeros((1, n_classes))\n",
        "\n",
        "        # Training loop\n",
        "        for ep in range(1, epochs + 1):\n",
        "            logits = X_train_vec @ self.W + self.b\n",
        "            probs = softmax(logits)\n",
        "\n",
        "            error = probs - y_train_onehot\n",
        "            dW = (X_train_vec.T @ error) / n_samples\n",
        "            db = np.mean(error, axis=0, keepdims=True)\n",
        "\n",
        "            self.W -= lr * dW\n",
        "            self.b -= lr * db\n",
        "\n",
        "            if ep % 10 == 0:\n",
        "                train_loss = -np.sum(y_train_onehot * np.log(probs + 1e-15)) / n_samples\n",
        "\n",
        "                val_logits = X_val_vec @ self.W + self.b\n",
        "                val_probs = softmax(val_logits)\n",
        "                val_loss = -np.sum(y_val_onehot * np.log(val_probs + 1e-15)) / n_val_samples\n",
        "\n",
        "                print(f\"Epoch {ep:3d} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"Kohli scored a century\", \"Nvidia launches NVLink 6\",\n",
        "                \"US President resigns\"]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [\"sports\", \"tech\", \"politics]\n",
        "        \"\"\"\n",
        "        X_vec = self.text_to_w2v(X_text)\n",
        "        logits = X_vec @ self.W + self.b\n",
        "        probs = softmax(logits)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def get_class_probabilities(self, X_text):\n",
        "        \"\"\"\n",
        "        Return probability distribution over classes.\n",
        "        \"\"\"\n",
        "        X_vec = self.text_to_w2v(X_text)\n",
        "        logits = X_vec @ self.W + self.b\n",
        "        return softmax(logits)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to directory\n",
        "        \"\"\"\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "        save_path = os.path.join(self.model_dir, \"w2v_classifier.pkl\")\n",
        "        state = {\n",
        "            \"W\": self.W,\n",
        "            \"b\": self.b\n",
        "        }\n",
        "        joblib.dump(state, save_path)\n",
        "        print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out different combinations of NUM_EPOCHS and LEARNING_RATE and check how\n",
        "# accuracy and convergence changes.\n",
        "\n",
        "NUM_EPOCHS = None # Number of epochs to train the model for\n",
        "LEARNING_RATE = None # Learning rate for training"
      ],
      "metadata": {
        "id": "ygbEo3j96F5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = f'word2vec_classifier'\n",
        "\n",
        "print(\"Loading Word2Vec classifier...\")\n",
        "w2v_clf = Word2VecClassifier(word2vec_path, model_dir)\n",
        "\n",
        "print(\"Training Word2Vec classifier...\")\n",
        "w2v_clf.fit(train_df, lr=LEARNING_RATE, epochs=NUM_EPOCHS)\n",
        "w2v_clf.save_model()\n",
        "\n",
        "X_test = test_df[\"text\"].values\n",
        "y_test = test_df[\"label\"].values\n",
        "\n",
        "w2v_predictions = w2v_clf.predict(X_test)\n",
        "w2v_accuracy = evaluate_classifier(y_test, w2v_predictions)\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "print(f\"Word2Vec Validation Accuracy: {w2v_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "LykyebxQGnSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAgjkxmvo3Wr"
      },
      "source": [
        "# Closing Note\n",
        "\n",
        "You report your findings to Timothy, close the final paper and look at your codebase.\n",
        "\n",
        "- You understand the methods.\n",
        "- You have implemented the methods from scratch.\n",
        "\n",
        "Timothy's newsroom is ready.\n",
        "\n",
        "And so are you."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}